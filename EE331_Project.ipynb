{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PardonMySkillz/PardonMySkillz/blob/main/EE331_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zv8rwJKRSWM"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n",
        "# 2023 Fall EE331 Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946wVZ9hRFLG"
      },
      "source": [
        "**OBJECTIVE**\n",
        "\n",
        "You are provided with a simple PyTorch model that classfies images into one of three categories: rock, paper or scissors. The current model uses just one fully connected (FC) layer for classfication. Your task is to do the following:\n",
        "\n",
        "1. Model Design\n",
        "\n",
        "\n",
        "> Replace the single FC layer in the '' class with a neural network architecture of your choice. Consider using combinations of layers such as multiple FC layers, convolutional layers, activation functions and dropout layers.\n",
        "\n",
        "\n",
        "2. Training\n",
        "\n",
        "\n",
        "> Once the model is designed,\n",
        "\n",
        "use the providced training loop to train the model. Adjust the hyperparameters if necessary. You may change the provided dataset with a dataset of your choice. However, this must be stated in the report you are going to write.\n",
        "\n",
        "\n",
        "You are free to change the code sections 1~????.\n",
        "\n",
        "For more detail on the report, refer to the project pdf file uploaded in KLMS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Cr79j25X-9"
      },
      "source": [
        "0. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VPrFX46oK-mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110d086a-3480-40d7-b195-d844dcb2e9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing_extensions import dataclass_transform\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# !pip install -Uqq ipdb\n",
        "# import ipdb\n",
        "# %pdb on\n",
        "\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QBzmLyOkduC",
        "outputId": "a31006e2-16a8-4f1f-95e0-66d919e8e082"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s4PwAKx8aiVd"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/archive 4.zip\", 'r')\n",
        "# zip_ref.extractall(\"/content/drive/MyDrive/archive 4\")\n",
        "# zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-V10BFRDar"
      },
      "source": [
        "**1. DATASET**\n",
        "\n",
        "The following link has some rock scissors paper dataset you can use:\n",
        "\n",
        "https://kaistackr-my.sharepoint.com/:f:/g/personal/shimjay17_kaist_ac_kr/EkBKx6r7RU1BnsFc5QbqFBMBHhOzF8SFoHXzaJAr1n7a3g?e=ElWBrF\n",
        "\n",
        "To use this dataset, you will have to upload these datasets to your google drive and specify its location in the dataset path in the code below.\n",
        "\n",
        "However, though it is not a necessity, we recommend that you test your model on additional datasets that are available to use. Remember that your model is going to be graded on an unseen custom dataset and will require your model good generalisation.\n",
        "\n",
        "If you do decide to use an additional dataset, you may need to edit the 'loadLabels' definition accordingly in the 'Dataset' class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZMgYYN9zLVtM"
      },
      "outputs": [],
      "source": [
        "\n",
        "######### Declare dataset path below! #########\n",
        "DATASET_PATH = '/content/drive/MyDrive/archive 2/Rock-Paper-Scissors/Rock-Paper-Scissors/train'\n",
        "###############################################\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, dataset_dir, transform=None, target_transform=None):\n",
        "        self.class_list = {'rock': 0, 'scissors': 1, 'paper': 2}\n",
        "        self.labels_map = {0:'rock', 1:'scissors', 2:'paper'}\n",
        "        self.path = DATASET_PATH\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(self.path, '**', '*.png'), recursive=True))\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()]) if transform is None else transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = self.loadImage(img_path)\n",
        "        image = cv2.resize(image, (28, 28))\n",
        "\n",
        "        # Apply the transform to the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.loadLabels(img_path)\n",
        "        return image, torch.tensor(label, dtype=torch.long)  # Ensure label is a long tensor\n",
        "\n",
        "    def loadImage(self, path):\n",
        "        img = cv2.imread(path)\n",
        "        img = img.astype(np.float32)/255.0  # 이미지\n",
        "        return img\n",
        "\n",
        "    def loadLabels(self, image_paths):\n",
        "        class_name = os.path.dirname(image_paths).split('/')[-1]\n",
        "        return self.class_list[class_name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPgmOpOzRMrm"
      },
      "source": [
        "**2. Classification Code**\n",
        "\n",
        "In this section, you are required to implement a network of your choice to get better performance.\n",
        "You are free to edit this skeleton code however you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V2eOsWxtNiho"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 3)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        print(np.shape(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CQzW4akNv9c"
      },
      "source": [
        "**3. Train & Evaluation Code**\n",
        "Here is an example Train and Evaluation code for the baseline FC layer model. You will most likely have to change the code below to fit your model of choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Um5qjYNhpWl-"
      },
      "outputs": [],
      "source": [
        "def train(train_dataset, val_dataset, model, device='cuda:0' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "    learning_rate = 0.001\n",
        "    epoch_no = 10\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    print(\"====TRAIN INITIATED====\")\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(epoch_no):\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for idx, data in enumerate(train_dataloader):\n",
        "            imgs, labels = data\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            output = model(imgs)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            model_loss = F.cross_entropy(output, labels)\n",
        "            running_loss += model_loss.item()\n",
        "            model_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_dataloader)\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "        print(f\"Epoch {epoch + 1} - Average training loss: {avg_train_loss:.3f}, Training accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # validation loop every 5 epochs\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            val_loss = 0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "            with torch.no_grad():\n",
        "                for val_idx, (val_imgs, val_labels) in enumerate(val_dataloader):\n",
        "                    val_imgs, val_labels = val_imgs.to(device), val_labels.to(device)\n",
        "                    val_output = model(val_imgs)\n",
        "\n",
        "                    _, val_predicted = torch.max(val_output.data, 1)\n",
        "                    total_val += val_labels.size(0)\n",
        "                    correct_val += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "                    val_loss += F.cross_entropy(val_output, val_labels)\n",
        "\n",
        "                val_accuracy = 100 * correct_val / total_val\n",
        "                print(f'Validation loss: {val_loss/(val_idx+1):.3f}, Validation accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "def test(test_dataset, model, device='cuda:0' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (test_imgs, test_labels) in enumerate(test_dataloader):\n",
        "            test_imgs, test_labels = test_imgs.to(device), test_labels.to(device)\n",
        "            test_output = model(test_imgs)\n",
        "\n",
        "            _, predicted = torch.max(test_output.data, 1)\n",
        "            total_test += test_labels.size(0)\n",
        "            correct_test += (predicted == test_labels).sum().item()\n",
        "\n",
        "            test_loss += F.cross_entropy(test_output, test_labels)\n",
        "\n",
        "        test_accuracy = 100 * correct_test / total_test\n",
        "        print(f'Test loss: {test_loss/(idx+1):.3f}, Test accuracy: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNtYWN6WN6al"
      },
      "source": [
        "**4. Run Code**\n",
        "\n",
        "Run the code below to run all the whole model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eJai2H_rN65E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce06c636-cd02-445f-92fb-66d73d810adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size : 1039\n",
            "Validation Data Size : 129\n",
            "Testing Data Size : 131\n",
            "====TRAIN INITIATED====\n",
            "Classifier(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 1 - Average training loss: 0.907, Training accuracy: 63.91%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 2 - Average training loss: 0.840, Training accuracy: 64.77%\n",
            "torch.Size([64, 400])\n",
            "torch.Size([64, 400])\n",
            "torch.Size([1, 400])\n",
            "Validation loss: 1.184, Validation accuracy: 60.47%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 3 - Average training loss: 0.754, Training accuracy: 65.64%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 4 - Average training loss: 0.597, Training accuracy: 75.26%\n",
            "torch.Size([64, 400])\n",
            "torch.Size([64, 400])\n",
            "torch.Size([1, 400])\n",
            "Validation loss: 0.637, Validation accuracy: 80.62%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 5 - Average training loss: 0.403, Training accuracy: 86.04%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 6 - Average training loss: 0.300, Training accuracy: 90.86%\n",
            "torch.Size([64, 400])\n",
            "torch.Size([64, 400])\n",
            "torch.Size([1, 400])\n",
            "Validation loss: 0.196, Validation accuracy: 93.80%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 7 - Average training loss: 0.164, Training accuracy: 95.38%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 8 - Average training loss: 0.084, Training accuracy: 97.79%\n",
            "torch.Size([64, 400])\n",
            "torch.Size([64, 400])\n",
            "torch.Size([1, 400])\n",
            "Validation loss: 0.196, Validation accuracy: 97.67%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 9 - Average training loss: 0.051, Training accuracy: 99.04%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([15, 400])\n",
            "Epoch 10 - Average training loss: 0.039, Training accuracy: 99.13%\n",
            "torch.Size([64, 400])\n",
            "torch.Size([64, 400])\n",
            "torch.Size([1, 400])\n",
            "Validation loss: 0.030, Validation accuracy: 99.22%\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([32, 400])\n",
            "torch.Size([3, 400])\n",
            "Test loss: 0.063, Test accuracy: 96.95%\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    dataset = Dataset(DATASET_PATH)\n",
        "    dataset_size = len(dataset)\n",
        "    train_size = int(dataset_size * 0.8)\n",
        "    validation_size = int(dataset_size * 0.1)\n",
        "    test_size = dataset_size - train_size - validation_size\n",
        "\n",
        "    train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
        "    print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
        "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
        "\n",
        "    model = Classifier()\n",
        "    train(train_dataset, validation_dataset, model)\n",
        "    test(test_dataset, model)\n",
        "\n",
        "    PATH = './WongHoiChun_20236391.pth'\n",
        "    torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nt-yivD96LX"
      },
      "source": [
        "**5. Inference**\n",
        "\n",
        "Once you are satisfied with your classifier code, you are to edit the code below so that it runs. The code below should successfully be able to load the 'test_image.jpeg' within the 'Project items' folder in the link given above and give a prediction for the image using your proposed model.\n",
        "\n",
        "The final output should return either 'rock', 'scissors' or 'paper'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4EL-n8PK5LTR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "5df93740-fcf1-4d80-a58f-fcee84d6d9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400])\n",
            "tensor([0.9036, 0.0913, 0.0052])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5d135560621b>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m################ Edit Here ################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-40c6931d0c22>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (method, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!method!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!method!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
          ]
        }
      ],
      "source": [
        "chosen_img_path = '/content/drive/MyDrive/test_image.jpeg'\n",
        "\n",
        "################ Edit Here ################\n",
        "\n",
        "\n",
        "\n",
        "################ Edit Here ################\n",
        "\n",
        "output = model(input)\n",
        "print(output)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "374acf0a8600279850f148506a6b4762afb2721aa53d2847250d0f3e4a044f03"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}